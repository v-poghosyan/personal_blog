[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vahram's Blog",
    "section": "",
    "text": "Introduction to Optimization for ML\n\n\n\n\n\n\n\noptimization\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2022\n\n\nVahram Poghosyan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello there ðŸ‘‹, Iâ€™m Vahram, a software engineer and graduate student of Computer Science at UT Austin with a deep-rooted passion for mathematics of machine learning and the applied sciences in general.\nThis blog is where I write about the subjects that interest me. Itâ€™s where I collect my notes and ideas on the web.\nI started it in late 2021 from some notes I took with Jupyter. From there it was only a small effort to publish them via Fastpages, a blogging platform powered by Jekyll. In 2023, I migrated the blog from the now deprecated Fastpages to Quarto.\n\n\n\n\n\n\nDisclaimer\n\n\n\n\n\nEverything in this blog, including the views I express, represent me â€” not the organization(s) I am affiliated with.\n\n\n\n\nðŸŽ“ Education\nUniversity of Texas at Austin\nM.S in Computer Science | In progress\nUniversity of California, Los Angeles\nB.S. in Applied Mathematics | 2020\n\n\nðŸ’» Experience\nCapital One | Software Engineer | Feb 2023 - Present\nCapital One | Software Intern | Jun 2022 - Aug 2022\nOmron Automation | Software Intern | Aug 2015 - Feb 2017"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/optimization/index.html",
    "href": "posts/optimization/index.html",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/index.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/index.html#model-of-a-convex-optimization-problem",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/index.html#why-convex-optimization",
    "href": "posts/optimization/index.html#why-convex-optimization",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/index.html#convexity",
    "href": "posts/optimization/index.html#convexity",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "Convexity",
    "text": "Convexity\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\) (i.e.Â the parametrized line segment between \\(x_1\\) and \\(x_2\\)) is also in \\(C\\). \n\n\nSome Operations that Preserve Convexity\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/optimization/index.html#examples-of-convex-sets",
    "href": "posts/optimization/index.html#examples-of-convex-sets",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "Examples of Convex Sets",
    "text": "Examples of Convex Sets\nThe following are some common convex sets we will come across in practice.\n\nConvex Hull of \\(n\\) Points\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\nConvex Hull of a Set\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\nAffine Combination of \\(n\\) Points\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\nLinear Combinations - Hyperplanes and Halfspaces\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\nHyperplanes\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\nHalfspaces\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\nConic Combinations of \\(n\\) Points\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\nEllipses\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\nNorm Balls\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\nPolyhedra\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\nThe Set of All Positive Semidefinite Matrices\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/optimization/index.html#footnotes",
    "href": "posts/optimization/index.html#footnotes",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/index.html",
    "href": "posts/optimization/intro_to_optimization/index.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/index.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/intro_to_optimization/index.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/index.html#why-convex-optimization",
    "href": "posts/optimization/intro_to_optimization/index.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/index.html#convexity",
    "href": "posts/optimization/intro_to_optimization/index.html#convexity",
    "title": "Introduction to Optimization for ML",
    "section": "Convexity",
    "text": "Convexity\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\nSome Operations that Preserve Convexity\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/index.html#examples-of-convex-sets",
    "href": "posts/optimization/intro_to_optimization/index.html#examples-of-convex-sets",
    "title": "Introduction to Optimization for ML",
    "section": "Examples of Convex Sets",
    "text": "Examples of Convex Sets\nThe following are some common convex sets we will come across in practice.\n\nConvex Hull of \\(n\\) Points\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\nConvex Hull of a Set\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\nAffine Combination of \\(n\\) Points\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\nLinear Combinations - Hyperplanes and Halfspaces\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\nHyperplanes\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\nHalfspaces\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\nConic Combinations of \\(n\\) Points\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\nEllipses\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\nNorm Balls\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\nPolyhedra\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\nThe Set of All Positive Semidefinite Matrices\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/index.html#footnotes",
    "href": "posts/optimization/intro_to_optimization/index.html#footnotes",
    "title": "Introduction to Optimization for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/linalg/intro_to_linalg/index.html",
    "href": "posts/linalg/intro_to_linalg/index.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg/index.html#convexity",
    "href": "posts/linalg/intro_to_linalg/index.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg/index.html#examples-of-convex-sets",
    "href": "posts/linalg/intro_to_linalg/index.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg/index.html#footnotes",
    "href": "posts/linalg/intro_to_linalg/index.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/linalg/intro_to_linalg/intro_to_linalg.html",
    "href": "posts/linalg/intro_to_linalg/intro_to_linalg.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg/intro_to_linalg.html#convexity",
    "href": "posts/linalg/intro_to_linalg/intro_to_linalg.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg/intro_to_linalg.html#examples-of-convex-sets",
    "href": "posts/linalg/intro_to_linalg/intro_to_linalg.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg/intro_to_linalg.html#footnotes",
    "href": "posts/linalg/intro_to_linalg/intro_to_linalg.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/intro_to_optimization.html",
    "href": "posts/optimization/intro_to_optimization/intro_to_optimization.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/intro_to_optimization/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization/intro_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization/intro_to_optimization/intro_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization.html",
    "href": "posts/optimization/intro_to_optimization.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/intro_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization/intro_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg.html",
    "href": "posts/linalg/intro_to_linalg.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg.html#convexity",
    "href": "posts/linalg/intro_to_linalg.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg.html#examples-of-convex-sets",
    "href": "posts/linalg/intro_to_linalg.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg/intro_to_linalg.html#footnotes",
    "href": "posts/linalg/intro_to_linalg.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/linear algebra/intro_to_linalg.html",
    "href": "posts/linear algebra/intro_to_linalg.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linear algebra/intro_to_linalg.html#convexity",
    "href": "posts/linear algebra/intro_to_linalg.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linear algebra/intro_to_linalg.html#examples-of-convex-sets",
    "href": "posts/linear algebra/intro_to_linalg.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linear algebra/intro_to_linalg.html#footnotes",
    "href": "posts/linear algebra/intro_to_linalg.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/Linear Algebra/intro_to_linalg.html",
    "href": "posts/Linear Algebra/intro_to_linalg.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/Linear Algebra/intro_to_linalg.html#convexity",
    "href": "posts/Linear Algebra/intro_to_linalg.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/Linear Algebra/intro_to_linalg.html#examples-of-convex-sets",
    "href": "posts/Linear Algebra/intro_to_linalg.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/Linear Algebra/intro_to_linalg.html#footnotes",
    "href": "posts/Linear Algebra/intro_to_linalg.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/optimization copy 3/intro_to_optimization.html",
    "href": "posts/optimization copy 3/intro_to_optimization.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization copy 3/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization copy 3/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization copy 3/intro_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization copy 3/intro_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization copy/intro_to_optimization.html",
    "href": "posts/optimization copy/intro_to_optimization.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization copy/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization copy/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization copy/intro_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization copy/intro_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization copy 2/intro_to_optimization.html",
    "href": "posts/optimization copy 2/intro_to_optimization.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization copy 2/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization copy 2/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization copy 2/intro_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization copy 2/intro_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization copy 4/intro_to_optimization.html",
    "href": "posts/optimization copy 4/intro_to_optimization.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization copy 4/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization copy 4/intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization copy 4/intro_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization copy 4/intro_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About this blog",
    "section": "ðŸŽ“ Education",
    "text": "ðŸŽ“ Education\nUniversity of Texas at Austin\nM.S in Computer Science | In progress\nUniversity of California, Los Angeles\nB.S. in Applied Mathematics | 2020"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About this blog",
    "section": "ðŸ’» Experience",
    "text": "ðŸ’» Experience\nCapital One | Software Engineer | Feb 2023 - Present\nCapital One | Software Intern | Jun 2022 - Aug 2022\nOmron Automation | Software Intern | Aug 2015 - Feb 2017"
  },
  {
    "objectID": "posts/intro_to_linalg.html",
    "href": "posts/intro_to_linalg.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/intro_to_linalg.html#convexity",
    "href": "posts/intro_to_linalg.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/intro_to_linalg.html#examples-of-convex-sets",
    "href": "posts/intro_to_linalg.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/intro_to_linalg.html#footnotes",
    "href": "posts/intro_to_linalg.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/optimization_intro_to_optimization.html",
    "href": "posts/optimization_intro_to_optimization.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization_intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization_intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization_intro_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization_intro_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/linalg_intro_to_linear_algebra.html",
    "href": "posts/linalg_intro_to_linear_algebra.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg_intro_to_linear_algebra.html#convexity",
    "href": "posts/linalg_intro_to_linear_algebra.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linalg_intro_to_linear_algebra.html#examples-of-convex-sets",
    "href": "posts/linalg_intro_to_linear_algebra.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg_intro_to_linear_algebra.html#footnotes",
    "href": "posts/linalg_intro_to_linear_algebra.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/linalg/linalg_intro_to_linear_algebra.html",
    "href": "posts/linalg/linalg_intro_to_linear_algebra.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg/linalg_intro_to_linear_algebra.html#convexity",
    "href": "posts/linalg/linalg_intro_to_linear_algebra.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/linalg/linalg_intro_to_linear_algebra.html#examples-of-convex-sets",
    "href": "posts/linalg/linalg_intro_to_linear_algebra.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "posts/linalg/linalg_intro_to_linear_algebra.html#footnotes",
    "href": "posts/linalg/linalg_intro_to_linear_algebra.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/optimization/optimization_intro_to_optimization.html",
    "href": "posts/optimization/optimization_intro_to_optimization.html",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e.Â minimize or maximize) some objective function. The objective function can be almost anything â€” cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s). Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/optimization_intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/optimization_intro_to_optimization.html#model-of-a-convex-optimization-problem",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/optimization_intro_to_optimization.html#why-convex-optimization",
    "href": "posts/optimization/optimization_intro_to_optimization.html#why-convex-optimization",
    "title": "Introduction to Optimization for ML",
    "section": "",
    "text": "First, letâ€™s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems â€” problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "linalg/linalg_intro_to_linear_algebra.html",
    "href": "linalg/linalg_intro_to_linear_algebra.html",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "We start our exploration of mathematics for machine learning with a refresher on convexity and, in general, the linear algebra thatâ€™s commonly used in the subject.\n\n\nSet convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex.\n\n\n\n\nThe following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "linalg/linalg_intro_to_linear_algebra.html#convexity",
    "href": "linalg/linalg_intro_to_linear_algebra.html#convexity",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "Set convexity is defined as follows:\n\nDefinition: Â  A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\)\n\nThat is, a set is convex if the parametrized line segment between \\(x_1\\) and \\(x_2\\), any two points in the set, is also entirely within the set. \n\n\nScaling, skewing, and rotation (i.e.Â linear transformations) preserve convexity, as do affine transformations (i.e.Â shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b : x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x : x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "linalg/linalg_intro_to_linear_algebra.html#examples-of-convex-sets",
    "href": "linalg/linalg_intro_to_linear_algebra.html#examples-of-convex-sets",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "",
    "text": "The following are some common convex sets we will come across in practice.\n\n\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)â€™s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\n\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements thereâ€™s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, itâ€™s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique 1 smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points â€” itâ€™s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\n\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)â€™s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, itâ€™s the line that passes through them, and for three points itâ€™s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import AppLayout, FloatSlider\n%matplotlib widget\n\n# # Slider\n# a1_slide = plt.axes([1.0, 0.1, 0.65, 0.03]) # x_pos, y_pos, width, height of slider/button \n# a1 = Slider(a1_slide, \"a1\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05) # slider, description, min_val, max_val, init_val, step\n# a2_slide = plt.axes([1.0, 0.2, 0.65, 0.03])\n# a2 = Slider(a2_slide, \"a2\", valmin=0.1, valmax=1, valinit=0.5, valstep=0.05)\n\n# # The function to be called anytime a slider's value changes\n# def update_a1(val):\n#     a2 =  1-a1\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# def update_a2(val):\n#     a1 =  1-a2\n#     p.set_ydata(x2(x1))\n#     fig.canvas.draw_idle()\n\n# # register the update function with each slider\n# a1.on_changed(update_a1)\n# a2.on_changed(update_a2)\n\n# # Affine combination with its weights\n# x1 = np.linspace(1,10,20)\n# x2 = lambda x1: 1/a2.val - (a1.val/a2.val)*x1\n\n# # Plot\n# fig = plt.figure()\n# ax = fig.subplots()\n# p = ax.plot(x1,x2(x1))\n# plt.show()\n\n# When using the `widget` backend from ipympl,\n# fig.canvas is a proper Jupyter interactive widget, which can be embedded in\n# an ipywidgets layout. See https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html\n\n# One can bound figure attributes to other widget values.\nfrom ipywidgets import AppLayout, FloatSlider\n\nplt.ioff()\n\nslider = FloatSlider(\n    orientation='horizontal',\n    description='Factor:',\n    value=1.0,\n    min=0.02,\n    max=2.0\n)\n\nslider.layout.margin = '0px 30% 0px 30%'\nslider.layout.width = '40%'\n\nfig = plt.figure()\nfig.canvas.header_visible = False\nfig.canvas.layout.min_height = '400px'\nplt.title('Plotting: y=sin({} * x)'.format(slider.value))\n\nx = np.linspace(0, 20, 500)\n\nlines = plt.plot(x, np.sin(slider.value * x))\n\ndef update_lines(change):\n    plt.title('Plotting: y=sin({} * x)'.format(change.new))\n    lines[0].set_data(x, np.sin(change.new * x))\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\nslider.observe(update_lines, names='value')\n\nAppLayout(\n    center=fig.canvas,\n    footer=slider,\n    pane_heights=[0, 6, 1]\n)\n\n\n\n\n\n\n\n\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)â€™s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n : \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\n\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x : a_1 x_1 + ... a_n x_n = b\\} = \\{ x : a^T x = b\\}\\)\nThereâ€™s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x : a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x : a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane thatâ€™s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which weâ€™ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\n\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x : a^T x = b\\}\\) are \\(\\{ x : a^T x \\geq b\\}\\) and ${ x : a^T x b} $.\n\n\n\n\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\n\n\n\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x : (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x : \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x : (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)â€™s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\n\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x : \\|x\\|_2 \\leq r \\}\\), and is clearly convex as itâ€™s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x : \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\n\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x : Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\n\nThe set of all PSD matrices \\(\\{ Q : x^TQx \\geq 0, \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a : x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q : x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q : x^TQx \\geq 0, \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity."
  },
  {
    "objectID": "linalg/linalg_intro_to_linear_algebra.html#footnotes",
    "href": "linalg/linalg_intro_to_linear_algebra.html#footnotes",
    "title": "Review of Linear Algebra and Geometry for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). But, any convex superset \\(D\\) of \\(C\\) must necessarily contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and similarly \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).â†©ï¸Ž"
  }
]