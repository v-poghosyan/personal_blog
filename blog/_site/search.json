[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nOptimization - Review of Linear Algebra and Geometry\n\n\n\n\n\n\n\noptimization\n\n\nlinear algebra\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2022\n\n\nVahram Poghosyan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/optimization/index.html",
    "href": "posts/optimization/index.html",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "",
    "text": "Optimization can be viewed as the attempt to find those parameter(s), if such exist, that optimize (i.e. minimize or maximize) some objective function. The objective function can be almost anything — cost, profit, number of nodes in a wireless network, distance to a destination, a similarity measure between two images, etc. If the objective function describes cost we may wish to minimize it. If, on the other hand, it describes profit then it would suit us to maximize it.\nThe problems of minimization and maximization, summed up as optimization in one word, are the same problem up to a reflection with respect to the axis (or domain) of the parameter(s).Formally, if the objective function is \\(f: \\mathbb{R^n} \\to \\mathbb{R}\\), and it has a minimizer \\(x^* \\in \\mathbb{R^n}\\). Then, by definition of minimizer, \\(f(x^*) \\leq f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\). It follows that \\(-f(x^*) \\geq -f(x) \\ \\ \\forall x \\in \\mathbb{R^n}\\), so \\(x^*\\) is a maximizer for \\(-f\\).\n\n\nThis post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post.\n\n\n\nFirst, let’s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/index.html#model-of-a-convex-optimization-problem",
    "href": "posts/optimization/index.html#model-of-a-convex-optimization-problem",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "",
    "text": "This post is the first in a series of posts on optimization. In the series, we frame an optimization problem in this form:\n\\[\\textrm{minimize}: f(x)\\] \\[\\textrm{subject to}: x \\in \\mathcal{X}\\]\nwhere the objective function \\(f\\) is a convex function, and the constraint set \\(\\mathcal{X}\\) is a convex set.\nWe will not, however, go over the ways in which we can model a real-world problem as one of the given form in the first place. There are many creative ways of doing that, one of which you can read about in this post."
  },
  {
    "objectID": "posts/optimization/index.html#why-convex-optimization",
    "href": "posts/optimization/index.html#why-convex-optimization",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "",
    "text": "First, let’s define the size of an optimization problem as the dimensionality of the parameter \\(x\\) added to the number of the problem constraints.\nConvex optimization problems are a class of easy optimization problems — problems whose time and/or space complexity grows slowly with respect to problem size.\nThese problems are general enough to capture many scenarios of interest, even some that do not fall strictly into the convex category, but specific enough to be solvable through generic algorithms and numerical methods."
  },
  {
    "objectID": "posts/optimization/index.html#convexity",
    "href": "posts/optimization/index.html#convexity",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "Convexity",
    "text": "Convexity\nSet convexity is defined as follows:\n\nDefinition:   A set \\(C \\subseteq \\mathbb{R^d}\\) is convex if, for all points \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), the point \\(\\theta x_1 + (1-\\theta) x_2\\) (i.e. the parametrized line segment between \\(x_1\\) and \\(x_2\\)) is also in \\(C\\). \n\n\nSome Operations that Preserve Convexity\nScaling, skewing, and rotation (i.e. linear transformations) preserve convexity, as do affine transformations (i.e. shifting). Let the matrix \\(A\\) define such a transformation, and \\(b\\) be a shift vector. Then \\(C' = \\{Ax + b \\ | \\ x \\in C \\}\\) is convex provided that \\(C\\) was convex.\nAn intersection of convex sets is also convex. That is, \\(C' = \\{ x \\ | \\ x \\in C_1 \\cap x \\in C_2 \\}\\) is convex provided that \\(C_1\\) and \\(C_2\\) were convex to begin with. The proof follows directly from the definition of intersection.\nHowever, unions of convex sets need not be convex."
  },
  {
    "objectID": "posts/optimization/index.html#examples-of-convex-sets",
    "href": "posts/optimization/index.html#examples-of-convex-sets",
    "title": "Optimization - Review of Linear Algebra and Geometry",
    "section": "Examples of Convex Sets",
    "text": "Examples of Convex Sets\nThe following are some common convex sets we will come across in practice.\n\nConvex Hull of \\(n\\) Points\n\nNote: A point and a vector mean the same thing for the purposes of the discussion that follows. \n\nA convex combination of points \\(x_1, ..., x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) where \\(\\sum_{i = 1}^{n} \\theta_i = 1\\) and \\(\\theta_i \\geq 0 \\ \\ \\forall i\\).\nLet \\(x_1,x_2,...,x_n\\) be \\(n\\) points in space. Their convex hull is the set of all points which can be written as some convex combination of them. Equivalently, by varying the \\(\\theta_i\\)’s we generate the convex hull as the set of all convex combinations of these points.\nThe convex hull can be visualized as the closed polygon formed when a rubber band is stretched around the \\(n\\) points. The convex hull of two points is the line segment joining them. That of three points is the polygon (complete with its inner region). In general, for \\(n\\) points, the concept generalizes to an \\(n\\)-dimensional polygon.\nFormally, the convex hull is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n \\ | \\ \\theta_1 + ... + \\theta_n = 1 \\ \\ \\textrm{and} \\ \\ \\theta_i \\geq 0 \\ \\ \\forall i \\}\\)\n\nNote: A handy interactive visualization, along with an efficient algorithm that generates a convex hull of \\(n\\) points on a 2D plane can be found in the following blog post by Joel Gibson. \n\n\n\nConvex Hull of a Set\nThe convex hull of a set can be similarly defined as all the convex combinations of the elements in the set. However, since the set may contain infinite elements there’s an equivalent definition in terms of supersets.\nLet \\(C\\) be a non-convex set. The convex hull of \\(C\\) is the intersection of all convex supersets of \\(C\\). That is, it’s the intersection of all convex sets containing \\(C\\). The result of such an intersection will be the unique {% fn 1 %} smallest convex superset of \\(C\\), its cinvex hull.\nVisualizing the convex hull of a non-convex set is similar to visualizing that of \\(n\\) points — it’s simply the shape enclosed by a rubber band stretched around the non-convex set.\n\n\nAffine Combination of \\(n\\) Points\nAn affine combination of points \\(x_1,...,x_n\\) is a point of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with \\(\\sum_{i=1}^{n}\\theta_i = 1\\) but where the \\(\\theta_i\\)’s need not be non-negative.\nFor a single point, the set of all affine combinations is the singleton set with the point itself. For two points, it’s the line that passes through them, and for three points it’s the plane. In general, it is the plane in \\(n\\)-dimensions passing through the \\(n\\) points.\n\n\nCode\nimport altair\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nrand = np.random.RandomState(42)\n\ndf = pd.DataFrame({\n    'xval': range(100),\n    'yval': rand.randn(100).cumsum()\n})\n\nprint(altair.__file__)\n\n\nslider = alt.binding_range(min=0, max=100, step=1)\ncutoff = alt.param(bind=slider, value=50)\n\n\n\nalt.Chart(df).mark_point().encode(\n    x='xval',\n    y='yval',\n    color=alt.condition(\n        alt.datum.xval &lt; cutoff,\n        alt.value('red'), alt.value('blue')\n    )\n).add_params(\n    cutoff\n)\n\n\nC:\\ProgramData\\Miniconda3\\envs\\machine-learning\\lib\\site-packages\\altair\\__init__.py\n\n\n\n\n\n\n\n\n\n\nLinear Combinations - Hyperplanes and Halfspaces\nA linear combination of \\(n\\) vectors, on the other hand, is all vectors of the form \\(x = \\theta_1 x_1 + ... + \\theta_n x_n\\) with the \\(\\theta_i\\)’s totally unrestricted.\nThe set of all linear combinations of \\(n\\) points is called their span. Formally, it is the set \\(\\{ \\theta_1 x_1 + ... + \\theta_n x_n \\ \\ | \\ \\ \\forall \\theta_1,...,\\theta_n \\}\\).\nThe span of a single point is the line passing through it. For two vectors the span is the plane passing through them and, in general, the span of \\(n\\) points is a plane in \\((n+1)\\)-dimensions which contains these points.\n\nHyperplanes\nFor fixed weights \\(\\theta_i = a_i \\ \\ \\forall i\\), a hyperplane is the set of all points \\(x \\in \\mathbb{R^n}\\) whose linear combination equals a fixed constant \\(b \\in \\mathbb{R}\\).\nFormally, a hyperplane is the set \\(\\{ x \\ \\ | \\ \\ a_1 x_1 + ... a_n x_n = b\\} = \\{ x \\ \\ | \\ \\ a^T x = b\\}\\)\nThere’s a geometric interpretation of the parameters \\(a \\in \\mathbb{R^n}\\) and \\(b \\in \\mathbb{R}\\). Since the dot-product between perpendicular vectors is \\(0\\), \\(\\{ x \\ \\ | \\ \\  a^T x = 0\\}\\) is simply the set of all vectors perpendicular to \\(a\\) (whose tail, as with all vectors in linear algebra, is considered to be fixed at the origin), making \\(a\\) the normal vector to the hyperplane passing through the origin. To allow for parallel hyperplanes that are translated from the origin, the offset \\(b \\in \\mathbb{R}\\) is introduced in the generalization \\(\\{ x \\ \\ | \\ \\ a^T x = b \\}\\). This is now the set of all vectors whose dot-product with \\(a\\) is constant. These vectors are not quite perpendicular to \\(a\\), but they form a parallel hyperplane that’s been shifted from the origin by a distance of \\(\\frac{|b|}{\\|a\\|_2}\\).\nSince the sum \\(a_1 x_1 + ... a_n x_n = b\\) is fixed, the last coordinate, which we’ll call \\(x_k\\) for some \\(k \\in [1,...,n]\\), is fixed by the choice of the other \\(n-1\\) coordinates. Therefore, a hyperplane in \\(\\mathbb{R^n}\\) spans \\(n-1\\) dimensions instead of \\(n\\). \n\n\nHalfspaces\nA halfspace is either of the two sub-spaces a hyperplane partitions the whole space into. Since the dot-product between vectors which are roughly in the same direction is positive, and vice versa, the two halfspaces associated to a hyperplane \\(\\{ x \\ \\ | \\ \\ a^T x = b\\}\\) are \\(\\{ x \\ \\ | \\ \\ a^T x \\geq b\\}\\) and ${ x   |   a^T x b} $.\n\n\n\nConic Combinations of \\(n\\) Points\nA conic combination of \\(x_1,...x_n\\) is a point \\(x = \\sum_{i=1}^{n} \\theta_i x_i\\) where \\(\\theta_i \\geq 0 \\ \\ \\forall i\\). Note that the absence of the restriction that \\(\\sum_{i=1}^{n} \\theta_i = 1\\) is what distinguishes a conic combination from a convex combination.\nA visual example:\n\n\n\nEllipses\nRecall from Euclidean geometry that ellipses are conic sections. In general we define ellipses in \\(n\\)-dimensions as the sub-level sets of quadratic forms. That is \\(\\{ x \\ \\ | \\ \\ (x-c)^T M (x-c) \\leq 1 \\}\\) where \\(M \\succeq 0\\) defines the stretch along each principal axis, and \\(c \\in \\mathbb{R^n}\\) is the center.\nAn equivalent definition of an ellipse using the L2-norm is \\(\\{ x \\ \\ | \\ \\ \\|Ax - b\\|_2 \\leq 1 \\}\\). That is, for a given \\(A\\) and \\(b\\) in the L2-norm definition, we can find an \\(M\\) and \\(c\\) in the sub-level set definition and vice versa.\n\nNote: More generally, the ellipse is \\(\\{ x \\ \\ | \\ \\ (x-c)^T M (x-c) \\leq r \\}\\). However, since the scaling factor \\(r\\) is positive, it can simply be absorbed into \\(Q\\) without affecting \\(Q\\)’s positive semidefiniteness. \n\nTo quickly convince ourselves in the equivalence of these definitions, we take the simple case where \\(b = 0\\).\n\\[\n\\begin{aligned}\n  \\|Ax\\|_2 &= ((Ax)^T(Ax))^{1/2} \\\\\n  &= (x^TA^TAx)^{1/2} \\\\\n  &= (x^TU D U^Tx)^{1/2} \\\\\n  &= x^TU D^{1/2} U^Tx \\\\\n  \\end{aligned}\n\\]\nWhere the third equality is by the spectral decomposition of the real symmetric matrix \\(A^TA\\), in which \\(D = diag(\\lambda_1,...,\\lambda_n)\\) is the diagnonal matrix of eigenvalues and the columns of \\(U\\) are the corresponding eigenvectors. Taking \\(M= UD^{1/2}U^T\\), where \\(D^{1/2}\\) is simply \\(D^{1/2} = diag(\\sqrt\\lambda_1,...,\\sqrt\\lambda_n)\\), we have the equivalent sub-level set definition of the ellipse.\n\n\nNorm Balls\nRelated to ellipses are Euclidean balls, which are norm balls for the choice of the L2-norm. A Euclidean ball has the form \\(\\{ x \\ \\ | \\ \\ \\|x\\|_2 \\leq r \\}\\), and is clearly convex as it’s a generalizations of the sphere in \\(n\\)-dimensions.\nBut also, a Euclidean ball is the special ellipse for the choice of \\(M = rI\\), and \\(c = 0\\).\nIn general, norm balls \\(\\{ x \\ \\ | \\ \\ \\|x\\|_p \\leq r\\}\\) where \\(\\|x\\|_p = (x_1^p + ... + x_n^p)^{1/p}\\) are convex for any choice of \\(p \\geq 1\\).\n\n\nPolyhedra\nWhere a halfspace is a set with one linear inequality constraint, a polyhedron is a set with many, but finite, such linear inequality constraints. These constraints can be packed into a matrix \\(A \\in \\mathbb{R^{m \\times n}}\\) by vector \\(b \\in \\mathbb{R^m}\\) multiplication form, making the polyhedron into the set \\(\\{x \\ \\ | \\ \\ Ax \\leq b\\}\\).\nSince polyhedra are simply intersections of halfspaces and hyperplanes, and the latter are both convex, polyhedra are also convex sets.\n\n\nThe Set of All Positive Semidefinite Matrices\nThe set of all PSD matrices \\(\\{ Q \\ \\ | \\ \\ x^TQx \\geq 0 \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is convex. We can, of course, use the definition of convexity to show this. But, a more elucidative approach would be the following remark.\nNote that \\(Q \\mapsto x^TQx\\) is a linear functional that maps the space of all PSD matrices to its field of scalars. This is analogous to how \\(a \\mapsto x^Ta\\) is a linear functional so, just as \\(\\{ a \\ \\ | \\ \\ x^Ta \\geq 0 \\}\\) is a halfspace in the space of vectors, \\(H_x = \\{ Q \\ \\ | \\ \\ x^TQx \\geq 0 \\}\\) for a given choice of \\(x \\in \\mathbb{R^m}\\) is a halfspace in the space of PSD matrices. Halfspaces, as we already know, are convex and \\(\\{ Q \\ \\ | \\ \\ x^TQx \\geq 0 \\ \\ \\forall x \\in \\mathbb{R^m}\\}\\) is nothing but an intersection of halfspaces for each choice of \\(x\\). That is, \\(\\{ Q \\ \\ | \\ \\ x^TQx \\geq 0 \\ \\ \\forall x \\in \\mathbb{R^m}\\} = \\bigcap_x H_x\\), concluding the proof of its convexity.\n{{ ‘Proof of uniqueness of the minimal, convex superset: Suppose \\(C_1\\) and \\(C_2\\) are both minimal, convex supersets of \\(C\\). Any convex set \\(D\\) that contains \\(C\\) must clearly contain the minimal, convex superset. Hence, \\(C_1 \\subseteq C_2\\) and \\(C_2 \\subseteq C_1\\), which implies \\(C_1 = C_2\\).’ | fndetail: 1 }}"
  }
]